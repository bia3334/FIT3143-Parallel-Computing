# FIT3143 - Parallel Computing
## Monash University Course Projects

A comprehensive collection of parallel computing implementations and performance analysis for FIT3143 (Parallel Computing) course.

## ğŸ“š Course Overview

This repository contains practical implementations exploring various parallel computing paradigms, performance optimization techniques, and comparative analysis across different parallelization approaches.

**Course:** FIT3143 - Parallel Computing  
**Institution:** Monash University  
**Academic Year:** S2-2025

## ğŸ—‚ï¸ Project Structure

```
FIT3143-Parallel-Computing/
â”œâ”€â”€ README.md                    # This file - Project overview
â”œâ”€â”€ Lab2/                        # âœ… Threads & OpenMP
â”‚   â”œâ”€â”€ README.md               # Lab-specific documentation
â”‚   â”œâ”€â”€ Lab2_Report.md          # Comprehensive technical report
â”‚   â”œâ”€â”€ task1.c                 # Serial implementation
â”‚   â”œâ”€â”€ task2.c                 # POSIX Threads implementation
â”‚   â”œâ”€â”€ task3.c                 # OpenMP implementation
â”‚   â”œâ”€â”€ primes_*.txt            # Output files
â”‚   â””â”€â”€ compiled executables
â”œâ”€â”€ Lab3/                        # âœ… Message Passing Interface (MPI)
â”‚   â”œâ”€â”€ README.md               # Lab-specific documentation
â”‚   â”œâ”€â”€ helloworld.c            # Basic MPI Hello World
â”‚   â”œâ”€â”€ task2a.c                # MPI Send/Recv communication
â”‚   â”œâ”€â”€ task2b.c                # MPI Broadcast communication
â”‚   â”œâ”€â”€ task3.c                 # MPI custom datatypes
â”‚   â”œâ”€â”€ task4.c                 # MPI Pack/Unpack
â”‚   â”œâ”€â”€ task5.c                 # Parallel Pi calculation
â”‚   â”œâ”€â”€ serial_pi.c             # Serial Pi for comparison
â”‚   â”œâ”€â”€ Makefile                # Build configuration
â”‚   â””â”€â”€ compiled executables
â”œâ”€â”€ Lab4/                        # ğŸš§ [Future Lab - TBD]
â””â”€â”€ Lab5/                        # ğŸš§ [Future Lab - TBD]
```

## ğŸ§ª Completed Labs

### Lab 2: Threads & OpenMP - Prime Number Finding âœ…
**Status:** Completed  
**Focus:** Comparative study of parallel computing approaches

**Implementations:**
- Serial baseline algorithm
- POSIX Threads (manual threading)
- OpenMP (high-level parallelization)

**Key Results:**
- **Best Performance:** OpenMP with 8 threads (3.00x speedup)
- **Worst Performance:** POSIX Threads (0.45x-0.65x - performance degradation)
- **Algorithm:** Prime number detection up to n=1,000,000

**Files:** `task1.c`, `task2.c`, `task3.c`, comprehensive documentation

[ğŸ“– View Lab 2 Details â†’](./Lab2/README.md)

### Lab 3: Message Passing Interface (MPI) âœ…
**Status:** Completed  
**Focus:** Distributed memory parallel computing with MPI

**Implementations:**
- Basic MPI Hello World program
- Point-to-point communication (Send/Recv)
- Collective communication (Broadcast)
- Custom MPI datatypes and data packing
- Parallel Pi approximation using numerical integration

**Key Results:**
- **Best Performance:** Pi calculation with 4 processes (250.4x speedup for N=1M)
- **Communication Patterns:** Collective operations outperform point-to-point
- **Algorithm:** Numerical integration of 4/(1+xÂ²) from 0 to 1

**Files:** `helloworld.c`, `task2a.c`, `task2b.c`, `task3.c`, `task4.c`, `task5.c`, `serial_pi.c`

[ğŸ“– View Lab 3 Details â†’](./Lab3/README.md)

## ğŸš§ Upcoming Labs

### Lab 4: [Topic TBD]
**Status:** Pending  
**Expected Focus:** [Will be updated when lab content is available]

### Lab 5: [Topic TBD]
**Status:** Pending  
**Expected Focus:** [Will be updated when lab content is available]

## ğŸ¯ Learning Objectives

Throughout this course, the following parallel computing concepts are explored:

### Core Concepts
- **Threading Models:** POSIX Threads vs OpenMP
- **Message Passing:** MPI communication patterns and collective operations
- **Performance Analysis:** Speed-up, efficiency, scalability metrics
- **Load Balancing:** Static vs dynamic task distribution
- **Memory Management:** Shared vs distributed memory architectures
- **Synchronization:** Critical sections, race conditions, deadlocks
- **Data Distribution:** Point-to-point vs collective communication strategies

### Technical Skills
- **Algorithm Parallelization:** Converting serial to parallel algorithms
- **MPI Programming:** Distributed computing with message passing
- **Communication Optimization:** Choosing efficient MPI operations
- **Performance Profiling:** Timing, bottleneck identification
- **Optimization Techniques:** Cache locality, NUMA awareness
- **Comparative Analysis:** Multi-approach evaluation methodology

## ğŸ”§ Development Environment

### System Requirements
- **OS:** Linux (tested on WSL2)
- **Compiler:** GCC with OpenMP support, MPI compiler (mpicc)
- **MPI Implementation:** OpenMPI or MPICH
- **Architecture:** x86_64
- **Cores:** Multi-core system recommended (tested on 20-core system)

### Setup Instructions
```bash
# Install required packages
sudo apt update
sudo apt install gcc build-essential openmpi-bin openmpi-common libopenmpi-dev

# Verify OpenMP support
gcc --version
echo | gcc -fopenmp -dM -E - | grep -i openmp

# Verify MPI installation
mpicc --version
mpirun --version
```

### Common Compilation Patterns
```bash
# Serial programs
gcc -o program program.c -lm

# POSIX Threads
gcc -o program program.c -lpthread -lm

# OpenMP
gcc -o program program.c -fopenmp -lm

# MPI programs
mpicc -o program program.c -lm

# With optimization (recommended for performance testing)
gcc -O3 -o program program.c -fopenmp -lm
mpicc -O3 -o program program.c -lm
```

## ğŸ“Š Performance Methodology

### Standard Testing Approach
1. **Baseline Establishment:** Serial implementation timing
2. **Parallel Implementations:** Multiple approaches tested
3. **Scalability Analysis:** Various thread counts evaluated
4. **Statistical Validity:** Multiple runs, average results
5. **Comparative Analysis:** Speed-up and efficiency calculations

### Metrics Used
- **Execution Time:** Wall-clock time measurement
- **Speed-up:** T_serial / T_parallel
- **Efficiency:** Speed-up / Number_of_threads Ã— 100%
- **Scalability:** Performance trends across thread counts

## ğŸ† Key Findings Summary

### Lab 2 Highlights
- **OpenMP demonstrates superior performance** over manual threading
- **Dynamic scheduling outperforms static partitioning** for irregular workloads
- **Optimal thread count** doesn't always equal available cores
- **Memory management strategy** critically impacts parallel performance

### Lab 3 Highlights
- **Collective operations (MPI_Bcast) outperform point-to-point** communication
- **MPI custom datatypes** provide elegant solution for complex data structures
- **Exceptional speedup results** (250x) due to cache effects and memory bandwidth utilization
- **Domain decomposition** scales effectively for numerical integration problems

*[Additional findings will be added as more labs are completed]*

## ğŸ“ˆ Course Progress

- [x] **Lab 2:** Threads & OpenMP (Completed)
- [x] **Lab 3:** Message Passing Interface (Completed)
- [ ] **Lab 4:** [Pending]
- [ ] **Lab 5:** [Pending]

## ğŸ”— Quick Navigation

| Lab | Topic | Status | Key Files | Performance Highlights |
|-----|-------|--------|-----------|----------------------|
| [Lab 2](./Lab2/) | Threads & OpenMP | âœ… Complete | `task1.c`, `task2.c`, `task3.c` | 3.00x speedup (OpenMP) |
| [Lab 3](./Lab3/) | Message Passing Interface | âœ… Complete | `helloworld.c`, `task2a.c`, `task2b.c`, `task3.c`, `task4.c`, `task5.c` | 250.4x speedup (MPI Pi calc) |
| Lab 4 | TBD | ğŸš§ Pending | - | - |
| Lab 5 | TBD | ğŸš§ Pending | - | - |

## ğŸ“ Documentation Standards

Each lab maintains consistent documentation:
- **README.md:** Quick start guide and overview
- **Report.md:** Comprehensive technical analysis
- **Source code:** Well-commented implementations
- **Output files:** Sample results and verification data

## ğŸ¤ Academic Integrity

This repository contains academic work for educational purposes. All implementations are original work completed according to Monash University academic integrity guidelines.

**Note:** Code may be referenced for learning purposes, but direct copying for academic submissions violates academic integrity policies.

## ğŸ“§ Contact Information

**Student:** Tien Hung Nguyen
**Student ID:** 35499435
**Email:** tngu0534@student.monash.edu  
**Course:** FIT3143 - Parallel Computing  
**Institution:** Monash University

---

## ğŸ“„ License

Educational use only - Monash University FIT3143 Course Material

---

**Last Updated:** January 2025  
**Repository Status:** Active Development (Labs 2-3 Complete, Labs 4-5 Pending)
