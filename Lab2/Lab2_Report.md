# FIT3143 Lab 2 Report: Threads & OpenMP
## Finding Prime Numbers with Parallel Computing

**Student Name:** [Your Name]  
**Student ID:** [Your ID]  
**Email:** [Your Email]@student.monash.edu  
**Date:** [Current Date]

---

## Executive Summary

This report presents a comparative analysis of parallel computing approaches for finding prime numbers using POSIX Threads and OpenMP. The study evaluates performance, scalability, and implementation efficiency across different thread configurations on a system with 20 available CPU cores.

**Key Findings:**
- OpenMP demonstrates superior performance with up to 3.00x speedup
- POSIX Threads implementation shows performance degradation in all test cases
- Optimal performance achieved with 8 threads using OpenMP
- Dynamic scheduling significantly outperforms static partitioning

---

## 1. Introduction

### 1.1 Objectives
- Implement parallel algorithms using POSIX Threads and OpenMP
- Analyze performance characteristics and scalability
- Compare different parallelization approaches
- Evaluate the effectiveness of task distribution strategies

### 1.2 Problem Statement
Find all prime numbers strictly less than integer n (n = 1,000,000) using:
1. Serial implementation (baseline)
2. POSIX Threads parallel implementation
3. OpenMP parallel implementation

### 1.3 System Specifications
- **CPU Cores:** 20 available cores
- **Architecture:** x86_64
- **Operating System:** Linux (WSL2)
- **Compiler:** GCC with optimization flags

---

## 2. Methodology

### 2.1 Prime Number Algorithm
All implementations use the same core algorithm:
```c
int is_prime(int num) {
    if (num < 2) return 0;
    if (num == 2) return 1;
    if (num % 2 == 0) return 0;
    int limit = (int)sqrt(num);
    for (int i = 3; i <= limit; i += 2) {
        if (num % i == 0) return 0;
    }
    return 1;
}
```

### 2.2 Implementation Approaches

#### 2.2.1 Serial Implementation (Task 1)
- Straightforward sequential execution
- Single thread processes all numbers from 2 to n-1
- Establishes baseline performance metrics

#### 2.2.2 POSIX Threads Implementation (Task 2)
- **Strategy:** Static range partitioning
- **Memory Management:** Per-thread memory allocation
- **Synchronization:** No shared memory conflicts
- **Load Balancing:** Static division of work ranges

```c
// Range partitioning approach
int range = n / num_threads;
int start_num = t * range;
int end_num = (t == num_threads - 1) ? n : start_num + range;
```

#### 2.2.3 OpenMP Implementation (Task 3)
- **Strategy:** Dynamic scheduling with OpenMP directives
- **Memory Management:** Shared array with critical sections
- **Load Balancing:** Dynamic work distribution
- **Chunk Size:** 100 iterations per work unit

```c
#pragma omp parallel for schedule(dynamic, 100)
for (int i = 2; i < n; i++) {
    if (is_prime(i)) {
        #pragma omp critical
        {
            primes[count++] = i;
        }
    }
}
```

---

## 3. Performance Results

### 3.1 Experimental Data

| Method | Threads | Time (s) | Speed-up | Efficiency |
|--------|---------|----------|----------|------------|
| **Serial** | 1 | 0.0723 | 1.00x | 100% |
| **POSIX Threads** | 4 | 0.1169 | 0.62x | 15.5% |
| **POSIX Threads** | 8 | 0.1109 | 0.65x | 8.1% |
| **POSIX Threads** | 16 | 0.1595 | 0.45x | 2.8% |
| **OpenMP** | 4 | 0.0341 | 2.12x | 53% |
| **OpenMP** | 8 | 0.0241 | 3.00x | 37.5% |
| **OpenMP** | 16 | 0.0653 | 1.11x | 6.9% |

**Speed-up Formula:** Speed-up = T_serial / T_parallel  
**Efficiency Formula:** Efficiency = Speed-up / Number_of_threads Ã— 100%

### 3.2 Performance Analysis

#### 3.2.1 POSIX Threads Results
- **All configurations slower than serial implementation**
- **Performance degradation increases with thread count**
- **Maximum slowdown: 2.2x with 16 threads (0.45x speed-up)**

#### 3.2.2 OpenMP Results
- **Consistent performance improvements across all thread counts**
- **Peak performance: 3.00x speedup with 8 threads**
- **Diminishing returns beyond 8 threads**

---

## 4. Detailed Analysis

### 4.1 POSIX Threads Analysis

#### 4.1.1 Speed-up Analysis
**Q: What is the speed-up? Is it reasonable?**

The POSIX Threads implementation shows **negative speed-up** across all configurations:
- 4 threads: 0.62x (38% slower)
- 8 threads: 0.65x (35% slower)  
- 16 threads: 0.45x (55% slower)

**This is not reasonable** and indicates fundamental implementation issues.

#### 4.1.2 Scalability Analysis
**Q: How does speed-up change when increasing/decreasing threads?**

Performance **degrades progressively** with more threads:
- **Root cause:** Thread overhead exceeds computational benefits
- **Memory allocation:** Each thread performs separate malloc operations
- **Cache inefficiency:** Poor memory locality across thread boundaries
- **Context switching:** Excessive overhead with thread management

#### 4.1.3 Task Distribution Evaluation
**Q: Is the task distribution approach good?**

The static range partitioning approach has **significant limitations:**

**Problems:**
- **Load imbalance:** Prime density varies across number ranges
- **Memory fragmentation:** Multiple allocation calls hurt performance
- **Cache misses:** Threads work on distant memory regions
- **Poor scalability:** Static approach doesn't adapt to workload

**Recommended improvements:**
- Use shared memory allocation
- Implement work-stealing algorithms
- Apply dynamic load balancing
- Optimize memory access patterns

### 4.2 OpenMP Analysis

#### 4.2.1 Speed-up Analysis
**Q: What is the speed-up? Is it reasonable?**

OpenMP demonstrates **excellent speed-up** characteristics:
- 4 threads: 2.12x speedup (reasonable)
- 8 threads: 3.00x speedup (excellent)
- 16 threads: 1.11x speedup (positive but diminished)

**This is very reasonable** and demonstrates effective parallelization.

#### 4.2.2 Scalability Analysis
**Q: How does speed-up change with different thread counts?**

OpenMP shows **optimal scaling pattern:**
- **Performance increases** from 4 to 8 threads
- **Peak performance** at 8 threads
- **Diminishing returns** beyond 8 threads due to:
  - Memory bandwidth saturation
  - Increased synchronization overhead
  - NUMA effects on multi-socket systems

#### 4.2.3 Task Distribution Evaluation
**Q: Is the task distribution approach efficient?**

The dynamic scheduling approach is **highly efficient:**

**Advantages:**
- **Automatic load balancing:** Work redistributed dynamically
- **Optimized memory usage:** Single shared allocation
- **Runtime optimization:** Compiler and library optimizations
- **NUMA-aware scheduling:** Better memory locality

**Minor limitations:**
- **Critical section overhead:** Serialization during result storage
- **Memory contention:** Shared access to result array

---

## 5. Comparative Analysis

### 5.1 Performance Comparison

**Performance Ranking:**
1. ðŸ¥‡ **OpenMP with 8 threads:** 3.00x speedup (Best)
2. ðŸ¥ˆ **OpenMP with 4 threads:** 2.12x speedup (Very Good)
3. ðŸ¥‰ **OpenMP with 16 threads:** 1.11x speedup (Good)
4. **Serial implementation:** 1.00x baseline
5. **POSIX Threads (all configs):** 0.45x - 0.65x (Poor)

### 5.2 Key Insights

#### 5.2.1 Why OpenMP Outperforms POSIX Threads
1. **Superior load balancing** through dynamic scheduling
2. **Lower runtime overhead** with optimized thread management
3. **Better memory management** with shared allocations
4. **Compiler optimizations** specific to parallel loops
5. **NUMA-aware** thread placement and scheduling

#### 5.2.2 Optimal Configuration
- **8 threads provide peak performance** for this workload
- **Beyond 8 threads:** Diminishing returns due to overhead
- **Hardware utilization:** 40% of available cores optimal

### 5.3 Algorithm Characteristics Impact
- **Embarrassingly parallel problem:** Prime checking is independent
- **Irregular workload:** Prime density varies across ranges
- **Memory-bound aspects:** Large result arrays affect performance
- **Cache sensitivity:** Memory access patterns crucial

---

## 6. Conclusions and Recommendations

### 6.1 Main Conclusions

1. **OpenMP is significantly superior** for this type of parallel computing task
2. **POSIX Threads implementation requires fundamental redesign** to be viable
3. **Dynamic scheduling outperforms static partitioning** for irregular workloads
4. **Optimal thread count depends on workload characteristics**, not just core count
5. **Memory management strategy critically impacts performance**

### 6.2 Practical Recommendations

#### 6.2.1 For Production Use
- **Choose OpenMP** for embarrassingly parallel problems
- **Use 8 threads** for optimal performance on this hardware
- **Monitor for diminishing returns** when scaling thread count
- **Consider algorithm-specific optimizations** beyond parallelization

#### 6.2.2 For POSIX Threads Improvement
- **Redesign memory allocation strategy** (use shared arrays)
- **Implement dynamic work distribution** (work-stealing queues)
- **Optimize for cache locality** in task assignment
- **Reduce thread creation overhead** with thread pools

### 6.3 Future Work
- **Investigate vectorization** (SIMD) for additional performance gains
- **Explore GPU acceleration** for larger problem sizes
- **Analyze different scheduling strategies** in OpenMP
- **Study NUMA effects** on larger multi-socket systems

---

## 7. References and Tools

### 7.1 Development Environment
- **Compiler:** GCC with `-fopenmp`, `-lpthread`, `-lm` flags
- **Profiling:** Built-in timing functions (`clock()`, `omp_get_wtime()`)
- **Testing:** Multiple runs with different thread configurations

### 7.2 Code Files Submitted
- `task1.c` - Serial implementation
- `task2.c` - POSIX Threads implementation  
- `task3.c` - OpenMP implementation
- Output files: `primes_serial.txt`, `primes_parallel.txt`, `primes_openmp.txt`

### 7.3 Verification
All implementations correctly identify **78,498 prime numbers** less than 1,000,000, ensuring algorithmic correctness across all approaches.

---

## Appendix A: Performance Charts

```
Speed-up Comparison Chart:

OpenMP Performance:
4 threads  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 2.12x
8 threads  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 3.00x
16 threads â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.11x

POSIX Threads Performance:
4 threads  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.62x (slower)
8 threads  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.65x (slower)
16 threads â–ˆâ–ˆâ–ˆâ–ˆ 0.45x (slower)

Serial Baseline: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.00x
```

---

**Declaration:** This work was completed independently using the provided lab instructions and system resources. No AI tools were used during the implementation and testing phases.
